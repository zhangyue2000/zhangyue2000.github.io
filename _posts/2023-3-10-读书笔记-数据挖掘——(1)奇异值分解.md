---
layout: post
title: "<读书笔记> 数据挖掘——(1)奇异值分解"
date:   2023-3-10
tags: [读书笔记,Data Mining,SVD]
comments: true
author: Zhang Yue


---

该节内容摘自于《Data-driven Science and Engineering》Chapter 1 Singular Value Decomposition。奇异值分解SVD是最重要的矩阵分解方法之一，它能够提供一个数值稳定的矩阵分解方式，并用于各种其他数据方法。

-----------------

**1.1 Overview**

高维是在处理复杂系统中的数据时常见的难题。这些系统中可能包含了大量的测量数据集，如声音、影像等等。而在许多自然系统中，我们是可以观察到数据所表现出的主要的模式，这些模式可以通过低维attractor或manifold表征出来。就像在图片中，每一张图片都包含了大量的像素，因此也属于一种高维向量空间。然而，大部分图片都是可以被高度压缩的，意味着相关的信息可以通过一种更加低维的子空间表示。

而SVD便提供了一种系统性的方法来根据主导模式确定高维数据的低维近似。这个技术是完全数据驱动的，因为模式完全从数据中而来，而无需任何的专家知识或直觉。

**SVD定义**

假设存在一个大数据集$X \in C^{n \times m}$，$X$的每一列$x_k \in C^n$可能都是在仿真或是试验中得到的测量数据，索引$k$表示了不同测量组间的顺序。对于每一个复值矩阵$X\in C ^{n \times m}$，SVD都能提供一个**唯一**的矩阵分解：


$$
X = U\Sigma V^\star
$$


其中，$U \in C^{n \times n}$和$V \in C^{m \times m}$是**具有正交列的单式矩阵**（unitary matrix），满足$UU^\star = U^\star U = I$。而$\Sigma \in R^{n \times m}$是一个**对角线上有实数非负元素，而对角线外均为0**的矩阵（对于实值矩阵，$X^\star = X^T$）。

当$n \geq m$时，矩阵$\Sigma$在对角线上至多有$m$个非0元素，因此可以写作$\Sigma = \begin{bmatrix} \hat{\Sigma} \\ 0\end{bmatrix}$，因此SVD分解也可以写成以下形式：


$$
X = U \Sigma V^\star = 
\begin{bmatrix} \hat{U} \ \hat{U}^{\perp} \end{bmatrix}
\begin{bmatrix} \hat{\Sigma} \\ 0\end{bmatrix}
V^\star
=\hat{U}\hat{\Sigma}V^\star
$$


$\hat{U}^{\perp}$的列张成的向量空间与$\hat{U}$列张成的向量空间互补且正交。$U$的列称为$X$的左奇异值向量（left singular vectors），而$V$的列称为$X$的右奇异值向量（right singular vectors），$\hat{\Sigma}$的对角线元素（除0外）称为奇异值，并从大到小排列。$X$的阶便等于非零奇异值的数量。

---------

**1.2 Matrix approximation**

SVD最有用的性质可能就是它能够为矩阵$X$提供一个最优的低秩近似。事实上，SVD提供了一种低秩近似的体系，比如我们可以通过保留前$r$个奇异值和向量，忽视剩余部分，从而得到一个rank-r的近似。

![image.png](https://s2.loli.net/2023/03/10/uKpU8FgvBncDG5M.png)

**Therom 1**

对$X$的最优rank-r近似，在最小二乘法中，由rank-r SVD给出：


$$
\arg \min_{\tilde{X},s.t. rank(\bar{X})=r} \vert \vert X - \tilde{X} \vert \vert_F = \tilde{U}\tilde{\Sigma}\tilde{V}^\star
$$


此处$\tilde{U}$和$\tilde{V}$是$U$和$V$的前$r$列，而$\tilde{\Sigma}$为$\Sigma$的前$r \times r $子块。

通过这个方法，我们能得到$X$的r阶近似$\tilde{X}=\tilde{U} \tilde{\Sigma} \tilde{V}^\star$。因为$\Sigma$为对角矩阵，所以rank-r SVD近似能够以r个不同的rank-1矩阵之和的形式给出：


$$
\tilde{X} = \sum_{k=1}^r\sigma_k u_k v_k^\star = \sigma_1u_1v_1^\star +\sigma_2u_2v_2^\star + ... +\sigma_ru_rv_r^\star
$$


-----

**1.3 Principal component analysis (PCA)**

主成分分析（PCA）是SVD的主要用途之一。PCA是一种讲数据转换到新的坐标系上的线性变换。大方差数据有着重要的结构：方差最大的数据位于第一个轴（第一主成分），而主成分分析通过丢弃低方差主成分来降低维数。

**PCA计算**

假设初始数据矩阵为$X \in R^{n \times m}$


$$
X=\{x_1,x_2,...,x_m\}
$$


我们对其进行中心化：


$$
\tilde{X}= \begin{bmatrix} x_1 - \hat{\mu}_1,...,x_m - \hat{\mu}_m \end{bmatrix}
$$


再求其协方差矩阵：


$$
S= \frac{1}{n-1}\tilde{X}^T\tilde{X}
$$


由于$\tilde{X}^T\tilde{X}$为gram matrix，因此$S$也是gram matrix，所以有：


$$
Sw_i = \lambda_i w_i \ \ or \ \ S=W\Lambda W^T \ \ or \ \ SW = W\Lambda \ \ or \ \ W^TSW=\Lambda
$$


$\Lambda$中的特征值从大到小排列为：


$$
\lambda_1 > \lambda_2 > ... > \lambda_m
$$


特征向量$w_i$称为$S$的第$i$个主成分



![image.png](https://s2.loli.net/2023/03/10/O4ZB13TSmEWLdQ7.png)





