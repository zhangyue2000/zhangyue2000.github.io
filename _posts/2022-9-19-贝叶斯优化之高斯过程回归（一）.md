
## 写在最前面

​	这是我贝叶斯优化笔记系列的第一篇文章，本系列主要是对自己学习贝叶斯优化相关知识的一个记录，文章大多从数学角度出发，主要剖析算法的数学本质。写这个系列不仅是为了记录下学习时的想法，便于日后自己复习，同时也希望与诸君共勉，共同进步。

## 引言

​	在机器学习中，优化一直是研究的热点之一。随着神经网络的逐步兴起与发展，网络变得越来越复杂，同时也使训练也变得越来越耗时和耗资约。以热门的模型BERT为例，BERT是NLP领域相当强大的模型，但训练它却是十分地耗时的，在19年时，谷歌的研究员通过改进算法，并且使用1024块TPU同时训练，也要使用一个多小时，而这一个多小时耗费的资源是极其庞大的。所以以往靠人为地调式网络的参数的种种缺点也暴露出来，这种方式不仅需要多耗费很多资源，同时也难以调出最优的参数。于是，AutoML的想法被提出，我们需要一个算法，能够算出这个最优参数到底是什么。贝叶斯优化这一种用于求解表达式未知的函数的极值问题黑盒优化算法，便常被用于AutoML中。下面，我将介绍算法的相关细节。

### Section 1

​	作为第一篇文章，我将先介绍贝叶斯优化的大体过程。第一步，贝叶斯优化算法将先随机初始化一个点集，这些点对应的函数值已知。第二步，算法将通过使用高斯过程回归来从这几个已知点来推断整个函数的概率分布。第三步，获得概率分布后，再通过构建采样函数找到下一个需要获得函数值的点。加入新的点后，再从第二步循环。最后，在所有函数值中找到最小的值作为最优值。

​	从上面的过程中可以看出，高斯过程回归在其中扮演着重要的作用，所以本系列的开始，我们将学习高斯过程回归。在学习高斯过程回归的开始，我们将首先了解贝叶斯回归。所以本篇将从Section 2开始介绍贝叶斯线性回归。

### Section 2

​	贝叶斯线性回归同一般的线性回归的解法不同，该算法使用MAP的方法对问题分析并求解，主要分为两大部分，分别为Inference和Prediction。其中Inference为算法关键。整个模型假设数据$(x,y)$满足下式：

$$
\begin{aligned}
&f(x)=\omega^{\top} x=x^{\top} \omega \\
&y=f(x)+\varepsilon \\
&\varepsilon \sim N\left(0, \sigma^2\right) \\
\end{aligned}
$$


#### 2.1 Inference
所谓Inference（推理），便是从数据集中求得参数分布的概率。也就是求得$P(w|Data)$，所以将先使用贝叶斯公式对其进行变形.
 
$$
\begin{aligned}
P(\omega \mid D a+a)=P(\omega \mid x, Y)=\frac{P(\omega, Y \mid x)}{P(Y \mid x)} \stackrel{(a)}{=} \frac{P(Y \mid \omega, x) P(\omega)}{\int p(Y \mid \omega, x) P(\omega) d \omega} \\
\end{aligned}
$$

其中$(a)$处等式原应该写为$P(w,Y \mid x)=P(Y \mid w,x)P(w \mid x)$，但由于$x$对$w$的分布没有影响，故$P(w \mid x)=P(w)$。

$$
\begin{aligned}
&P ( Y \mid w, x)=\prod_{i=1}^N P\left(y_i \mid w_i, x_i\right) \stackrel{(a)}{=} \prod_{i=1}^N N\left(y_i \mid w^{\top} x_i, \sigma^2\right)
\end{aligned}
$$

其中$(a)$为根据模型假设而推得$P(y_i \mid w_i,x_i)=N(\omega^{\top} x_i,\sigma^2)$。

给出对参数的先验概率如下

$$
\begin{aligned}
&P(\omega)=N\left(0, \Sigma_p\right)\\
\end{aligned}
$$

根据

$$
P(\omega \mid D a t a) \propto P(Y \mid \omega, x) \cdot P(\omega) \propto \prod_{i=1}^N N\left(y_i \mid \omega^{\top} x_i, \sigma^2\right) \cdot N\left(0, \Sigma_p\right)\\
$$

以及高斯分布的自共轭性，可得

$$
\begin{aligned}
&P\left(w \mid Data\right) \sim N\left(\mu_w, \Sigma_w\right)\\
\end{aligned}
$$

对$P(Y \mid x,w)$求解分布如下

$$
\begin{aligned}
P(Y \mid x, \omega) &=\prod_{i=1}^N \frac{1}{(2 \pi)^{\frac{1}{2}} \sigma} \exp \left\{-\frac{1}{2 \sigma^2}\left(y_i-\omega^{\top} x_i\right)^2\right\} \\
&=\frac{1}{(2 \pi)^{\frac{N}{2}} \sigma^N} \exp \left\{\sum_{i=1}^N-\frac{1}{2 \sigma^2}\left(y_i-\omega^{\top} x_i\right)^2\right\} \\
&=\frac{1}{(2 \pi)^{\frac{N}{2}} \cdot \sigma^N} \exp \left\{-\frac{1}{2}\left(Y-X\omega\right)^{\top} \sigma^{-2}\left(Y-X\omega\right)\right\}
\end{aligned}
$$

可从中看出，

$$
P\left(Y|x, \omega) = N\left(x \omega, \sigma^{-2} I\right)\right.
$$

从而对后验有

$$
\begin{aligned}
P(w|Data) \propto P(Y \mid w, x) \cdot P(\omega) & \propto \exp \left\{-\frac{1}{2 \sigma^2}\left(Y^{\top}-w^{\top} x^{\top}\right)(Y-X \omega)-\frac{1}{2} w^{\top} \Sigma_p^{-1} w\right\} \\
&=\exp \left\{-\frac{1}{26^2}\left(Y^{\top} Y-2 Y^{\top} X w+w^{\top} x^{\top} x w\right)-\frac{1}{2} w^{\top} \Sigma^{-1} w\right\}
\end{aligned}
$$

由先前的推论，后验为高斯分布可知道，

$$
P\left(\omega \mid Da t a\right) \propto \exp \left\{-\frac{1}{2}\left(\omega-\mu_w\right)^{\top} \Sigma_w^{-1}\left(\omega-\mu_\omega\right)\right\}=\exp \left\{-\frac{1}{2}\left(\omega^{\top} \Sigma_\omega^{-1} \omega-2 \mu_\omega^{\top} {\Sigma}_\omega^{-1} \omega+\Delta\right)\right\}
$$

对比上面两个式子可以得到

$$
\begin{aligned}
&\Sigma_\omega=A^{-1} \quad\left(A=\sigma^{-2} x^{\top} x+\Sigma_p^{-1}\right) \\
&\mu_\omega=\sigma^{-2} A^{-1} x^{\top} Y
\end{aligned}
$$

从而得到了后验的概率分布，完成了Inference部分。

#### 2.2 Predicted

根据后验就可以对新的数据点的函数值按照下面的式子进行预测了。

$$
\begin{aligned}
&f\left(x^*\right)=x^{*^{\top}} \omega \quad y^*=f\left(x^*\right)+\varepsilon \\
&x^{*^{\top}} \omega \sim N\left(x^{*^{\top}} \mu_\omega, x^{* \top} \Sigma_\omega x^*+\sigma^2\right) \\
&P\left(y^* \mid D ata, x^*\right) \\
&=\int_\omega P\left(y^* \mid \omega, x^*\right) P(\omega \mid D ata) d \omega
\end{aligned}
$$

**至此，贝叶斯过程回归就到这里了，下一篇将从贝叶斯线性回归进入到高斯过程回归。**
