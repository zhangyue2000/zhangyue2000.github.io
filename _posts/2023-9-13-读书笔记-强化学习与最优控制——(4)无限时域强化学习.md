---
layout: post
title: "<读书笔记> 强化学习与最优控制——(4)无限时域强化学习-1"
date:   2023-9-13
tags: [读书笔记,MDP,RL,ADP]
comments: true
author: Zhang Yue


---

<a href="https://smms.app/image/4ZwvenYtO3ygQP1" target="_blank"><img src="https://s2.loli.net/2023/09/14/4ZwvenYtO3ygQP1.png" align="center"></a>


该节内容摘自于《Reinforcement Learning and Optimal Control》Chapter 4 An Overview of Infinite Horizon Problems。无限时域问题于有限时域问题主要在两个方面有所不同：

1.阶段的数量是无限的；

2.系统是stationary的，即system equation, the cost per stage, and the random disturbance statistics并不会从某个阶段开始变到下一个阶段。

当然在实践中无限数量的阶段的假设是不可能存在的，但是它可以作为一个当阶段数非常多情况下的合理近似。而stationary的假设在实践中经常得以满足，或者在一些情况下系统参数随时间变化会较为缓慢。

在无限时域问题中同样诞生了许多优雅的分析，并且它们的最优策略往往简单于有限时域下的同等问题。这正是由于stationary导致的，在长期过程中系统往往最后会进入一个较为稳定的状态，而每个阶段的最优决策也趋于一致。

但是在无限时域问题中也需要更为严谨的数学处理。

---------------------

**4.1 有限时域问题总览**

我们将会关注两类有限时域问题，它们的目标都是最小化无限数量阶段下的总成本

$$
J_{\pi}(x_0) = \lim_{N \rightarrow \infin} E_{w_k,k=0,1,...}
\left \{
    \sum_{k=0}^{N-1}
    \alpha^k
    g(x_k, \mu_k(x_k), w_k)

\right \}
$$

此处$J_{\pi}(x_0)$表示与初始状态$x_0$和策略$\pi=\{\pi_0, \pi_1,...\}$等相关的成本，$\alpha$为正系数。$\alpha$往往小于1，表示相较于当前时间带来的成本，未来的成本并没有当前时间的成本重要。（当前的决策越往后影响越小）

![image.png](https://s2.loli.net/2023/09/13/oB8Eigcjtf2qzIO.png)

而这两类时域问题中是随机最短路问题（SSP）和折扣问题。

大部分无限时域问题都围绕着相应的N阶段问题展开。例如在SSP问题中，假设$J_N(x)$表示N阶段问题的最优成本，初始状态为$x$，每个阶段成本为$g(x,u,w)$，最终成本为0。在N次迭代后，我们根据DP算法得到

$$
J_{k+1}(x) = \min_{u \in U(x)} E_w
\left \{
    g(x,u,w) + J_k(f(x,u,w))    
\right \}
,k=0,1,...
$$

该算法也被称之为值迭代算法。因此在无限时域下，一个给定策略的成本可以根据定义估算为

$$
J^\star(x) = \lim_{N \to \infty} J_N(x)
$$

因此，对值迭代算法左右两边同时取极限，我们可以得到下式

$$
J^\star(x) = \min_{u \in U(x)} E_w 
\left\{
    g(x,u,w)
    +
    J^\star(f(x,u,w))
\right\}
$$

如果$\mu(x)$在上式右侧对于任意的$x$都取得了最小值，那么策略$\{\mu,\mu,...\}$就是最优的。此类策略称之为stationary。

**无限时域问题的转移概率表示**

在本章中，我们假定了整个动态系统是有限状态、离散时间的。因此我们对这样的一个系统使用一种特别的转移概率表示。我们用$i$表示状态，$j$表示其后的状态。我们假定共有$n$个状态，从$1$索引到$n$，而终止状态标记为$t$。控制$u$只能在给定的有限约束集合$U(i)$中取值，该集合依赖于当前的状态$i$。在状态$i$处使用控制$u$指定了到下一个状态$j$的转移概率$p_{ij}(u)$，成本为$g(i,u,j)$。

给定一个可行的策略$\pi = \left\{ \mu_0, \mu_1, \cdots \right\}$（每个元素均满足$\mu_k(i) \in U(i)$）和初始状态$i_0$，系统就变成了一个根据$\pi$产生trajectory的马尔可夫链，标记为$\{ i_0,i_1, \cdots \}$。与初始状态$i$相关的总期望成本为
$$
J_{\pi}(i) = \lim_{N \to \infty} E 
\left \{ 
    \sum_{k=0}^N
    \alpha^k
    g
    \left( 
        i_k,\mu^k(i_k),i_{k+1}
    \right)
    \vert i_0 = i, \pi
\right \}
$$

将stationary策略$\pi = \left \{ \mu, \mu, \cdots \right \}$的cost function表示为$J_{\mu}(i)$。如果满足下列条件，则称$\pi$是最优的：

$$
J_{\mu}(i) = J^\star(i) = \min_\pi J_{\pi}(i)
$$

**4.2 随机最短路问题**

在SSP问题中，折扣是不存在的（$\alpha = 1$），并且存在特殊的cost-free终止状态$t$。一旦系统到达该状态后，就不再有后续成本，即
$$
p_tt(u) = 1, g(t,u,t)=0, \forall u \in U(t)
$$

<a href="https://smms.app/image/oMSNImyenKbh941" target="_blank" align="center"><img src="https://s2.loli.net/2023/09/14/oMSNImyenKbh941.png" ></a>

> SSP问题中的Bellman等式和值迭代
>
> 对于任意的$i=1,\cdots,n$，我们有
>
> $$
> J^\star(i) = \min_{u \in U(i)}
> \left\{
> p_{it}g(i,u,t) + \sum_{j = 1}^n p_{ij}(u)(g(i,u,j) + J^\star(j))
> \right\}
> $$
> 对于任意的$i=1,\cdots,n$和任意的初始条件$J_0(1),\cdots,J_0(n)$，值迭代算法根据下式产生序列$\{J_k\}$
> $$
> J_{k+1}(i) = \min_{u \in U(i)}
> \left [
> p_{it}(u)g(i,u,t) + \sum_{j = 1}^n p_{ij}(u)(g(i,u,j) + J_k(j))
> \right ]
> $$
注意到，在确定最短路问题中，其实只是对$p_{ij}(u)$添加了额外的约束：对于每一对状态控制对$(i,u)$，都能找到一个状态$j$使得$p_{ij}(u) = 1$

显然，到达终止状态$t$是不可避免的，所以问题的本质是如何以最低的期望成本到达状态$t$。在本章讨论SSP问题中，将做出以下假设来证明所有策略下最后都会终止。

> Assumption：存在整数$m$使得无论任何策略和初始状态，都存在为正数的概率使得终止状态能够在$m$阶段内到达，即对于所有可行策略$\pi$，我们有
> $$
> \rho_\pi = \max_{i = 1,\cdots,n}P\{x_m \neq t \vert x_0 = i, \pi \} \lt 1

假设$\rho$是不到达状态$t$的最大概率，即$\rho = \max_\pi \rho_\pi$。注意到，$\rho_\pi$仅依赖于策略$\pi$的前$m$个元素。而且，如果在每个状态处可用的控制数量是有限的，那么在$m$个阶段的不同的策略数量同样也是有限的。因此，$\rho_\pi$的值的情况也是有限的，所以$\rho \lt 1$。

并且，注意到对于任意的$\pi$和初始状态$i$，有
$$
\begin{align*}
P\left\{
x_{2m} \neq t \vert x_0 = i, \pi    
\right\}
= &
P\left\{
x_{2m} \neq t \vert x_{m} \neq t, x_0 = i, \pi    
\right\}
\\
& \cdot
P\left\{
x_{m} \neq t \vert x_0 = i, \pi    
\right\}
\\
\leq & \rho^2
\end{align*}
$$
因此，对于任意的$pi$，在$km$阶段后没有到达终止状态的概率像$\rho^k$一样下降
$$
P\left\{x_{km} \neq t \vert x_0 = i, \pi \right \} \leq \rho^k, i = 1, ..., n
$$
> Proposition 4.2.1：VI的收敛性
>
> 给定任意的初始条件$J_0(1),\cdots,J_0(n)$，根据值迭代算法得到的序列$\{J_k(i)\}$对于每个$i=1,\cdots,n$会收敛到最优成本$J^\star(i)$
> $$
> J_{k+1}(i) = \min_{u \in U(i)}
> \left[
> p_{it}(u)g(i,u,t)+\sum_{j=1}^np_{ij}(u)(g(i,u,j)+J_k(j))
> \right]
> $$
> Proposition 4.2.2：Bellman等式
>
> 最优成本函数$J^\star = (J^\star(1),\cdots,J^\star(n))$对于任意的$i=1,...n$，都满足下列式子，并且是该式的唯一解
> $$
> J^\star(i) = \min_{u \in U(i)}
> \left[
> p_{it}(u)g(i,u,t)+\sum_{j=1}^np_{ij}(u)(g(i,u,j)+J^\star(j))
> \right]
> $$
> Proposition 4.2.3：对于策略的值迭代与Bellman等式
>
> 对于任意的stationary策略$\mu$，相应的cost function$J_\mu = (J_{\mu}(1),\cdots,J_{\mu}(n))$对于所有的$i=1,\cdots,n$均满足下述等式且是该式的唯一解
> $$
> J_{\mu}(i) = p_{it}(\mu(i))g(i,\mu(i),t)+\sum_{j=1}^np_{ij}(\mu(i))\left(g(i,\mu(i),j)+J_\mu(j)\right)
> $$
> 并且，给定任意的初始条件$J_{0}(1),\cdots,J_{0}(n)$，根据值迭代算法采用策略$\mu$产生的序列$\{J_k(i)\}$
> $$
> J_{k+1}(i) = p_{it}(\mu(i))g(i,\mu(i),t) + \sum_{j=1}^np_{ij}(\mu(i))(g(i, \mu(i), j) + J_k(j))
> $$
> 将会收敛到成本$J_{\mu}(i)$

最后的结果为stationary policy的最优性提供了充分必要条件
> Proposition 4.2.4：最优性条件
> 一个stationary策略$\mu$是最优的当且仅当对于每一个状态$i$，$\mu(i)$都在Bellman等式中取得了最小值。
> $$
> J^\star(i) = \min_{u \in U(i)}
> \left[
> p_{it}(u)g(i,u,t)+\sum_{j=1}^np_{ij}(u)(g(i,u,j)+J^\star(j))
> \right]
> $$

**4.3 折扣问题**

在折扣问题中，$\alpha \lt 1$。
> 折扣问题中的Bellman等式和值迭代
>
> 对于所有的$i=1,\cdots,n$，我们有
> $$
> J^\star(i) = \min_{u \in U(i)} \sum_{j=1}^np_{ij}(u)
> \left(
> g(i,u,j) + \alpha J^\star(j)
> \right)
> $$
> 对于所有的$i=1,\cdots,n$和任意的初始条件$J_0(1),\cdots,J_0(n)$，值迭代算法根据下式产生序列$\{J_k\}$
> $$
> J_{k+1}(i) = \min_{u \in U(i)} \sum_{j=1}^n p_{ij}(u)
> \left(
> g(i,u,j) + \alpha J_k(j)
> \right)

接下来将会证明折扣问题也可以被转化为一个SSP问题。同样地，用$i=1,\cdots,n$表示状态，考虑一个相关的拥有状态$1,\cdots,n$和"人造"终止状态$t$的SSP问题。但在该SSP问题中，状态转移和成本解算按照如下进行：从状态$i \neq t$，应用动作$u$，则下一个状态是$j$的概率为$\alpha p_{ij}(u)$，成本为$g(i,u,j)$；下一个状态是$t$的概率为$1 - \alpha$，成本为0。

假定我们在折扣问题和SSP问题中采用相同的策略。那么，只要终止没有发生，两个问题中的状态演变是由相同的转移概率控制的。

![image.png](https://s2.loli.net/2023/09/15/mnP2B8paEX1zUKJ.png)

并且，相关的SSP问题的第$k$阶段的期望成本是$g(i_k,\mu^k(i_k),i_{k+1})$的期望值乘以还未到达状态$t$的概率$\alpha^k$。这也是折扣问题中第$k$阶段的期望成本。因此，对于折扣问题和相关的SSP问题，从给定状态开始的任何策略的成本都是相同的。

因此，在折扣问题中Proposition 4.2.1、4.2.2、4.2.3、4.2.4仍然适用。


